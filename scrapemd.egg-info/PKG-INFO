Metadata-Version: 2.2
Name: scrapemd
Version: 0.1.2
Summary: Web scraper that converts web pages to markdown
Home-page: https://github.com/yourusername/scraper
Author: Your Name
Author-email: your.email@example.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: setuptools>=65.0.0
Requires-Dist: beautifulsoup4>=4.11.0
Requires-Dist: html2text>=2020.1.16
Requires-Dist: crawl4ai>=0.5.0
Requires-Dist: requests>=2.28.0
Requires-Dist: keyring>=23.0.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Web Scraper with crawl4ai

A command-line application that takes a URL as input, scrapes the content of the webpage, and converts it into a markdown file. The application utilizes the crawl4ai library for the core scraping functionality.

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Content Processing Expectations](#content-processing-expectations)
- [Troubleshooting](#troubleshooting)
- [Logging Configuration](#logging-configuration)
- [Running Tests](#running-tests)
- [Deployment](#deployment)
- [Development](#development)
- [License](#license)

## Features

- Command-line interface that accepts URLs and various options
- Converts web content into well-formatted markdown
- Preserves tables and other complex elements
- Organizes output by domain and page title
- Special handling for Confluence pages
- Configurable crawl depth
- Comprehensive error handling and logging
- Robust content extraction with multiple fallback methods

## Installation

1. Clone the repository:
```
git clone https://github.com/yourusername/scraper.git
cd scraper
```

2. Install the package:
```
pip install -e .
```

This will install the package in development mode and create a `scrapemd` command that you can use from anywhere.

Alternatively, you can install just the dependencies:
```
pip install -r requirements.txt
```

3. Run post-installation setup:
```
python -m src.setup
```

4. For automated installation, you can use the deployment script:
```
./deploy.sh
```

This script will:
- Create a virtual environment if needed
- Install all dependencies
- Install the package in development mode
- Run tests if pytest is available
- Set up the application
- Test the command availability

## Usage

### Basic Usage

```
scrapemd --url https://example.com
```

### Additional Options

```
scrapemd --url https://example.com --output ./scraped_content/ --depth 2
```

### For Confluence Sites

```
scrapemd --url https://confluence.example.com --space SPACEKEY --username user --password pass
```

## Configuration

You can customize the scraper behavior by editing the config files in the `config` directory or by passing command-line arguments.

The default configuration file is located at `config/default_config.yml` and includes settings for:
- Output directory and file handling
- Crawler behavior (timeout, retries, user agent)
- Depth control
- Authentication
- Markdown formatting
- Logging

## Content Processing Expectations

### Expected Output Structure

When you run the scraper against a URL, it processes the page content and converts it to markdown format. Here's what to expect in the output:

#### Standard Output Structure

A typical markdown output file will have this structure:

```markdown
# Page Title

_Source: https://example.com/page_  
_Crawled: 2025-03-05 05:00:00_  

---

## Main Heading

Content paragraphs...

### Sub Heading

More content...

- List item 1
- List item 2

| Column 1 | Column 2 | Column 3 |
|----------|----------|----------|
| Data 1   | Data 2   | Data 3   |
| Data 4   | Data 5   | Data 6   |

[Link text](https://example.com/linked-page)

![Image description](https://example.com/image.jpg)
```

#### Output File Location

By default, scraped content is saved to:
- Base directory: `./scraped_content/`
- Domain subdirectory: `./scraped_content/example.com/`
- File naming: `[Page_Title]_[Timestamp].md`

For Confluence sites with space keys:
- `./scraped_content/confluence.example.com/SPACEKEY/[Page_Title]_[Timestamp].md`

### Element Processing

The scraper preserves the following HTML elements in markdown format:

- **Headings**: HTML headings become markdown headings with proper hierarchy
- **Paragraphs**: Text paragraphs with appropriate line breaks
- **Lists**: Both ordered and unordered lists with hierarchy
- **Tables**: HTML tables converted to markdown table format
- **Links**: Converted to markdown format `[link text](url)`
- **Images**: Referenced in markdown format `![alt text](image url)`
- **Code Blocks**: Preserved with proper formatting

### Content Extraction Methods

The processor uses multiple methods to extract content, in the following priority order:

1. **Markdown from crawl4ai**: Uses the built-in markdown conversion from crawl4ai
2. **Enhanced extraction content**: Uses enhanced extraction methods if available
3. **Standard content**: Falls back to standard content extraction
4. **Plain text**: Uses plain text if other methods fail
5. **HTML conversion**: Converts HTML to markdown using html2text or BeautifulSoup

For title extraction, the processor tries:
1. The title field from crawl4ai
2. HTML `<title>` tag
3. Open Graph meta tags
4. Main heading (`<h1>`)
5. Other prominent headings
6. URL path segments
7. Domain name as a last resort

### Handling Missing or Unexpected Content

#### Common Scenarios and Solutions

1. **Missing Page Title**
   - The scraper will use the URL's path as the title
   - Example: `https://example.com/about-us` â†’ `about-us_[timestamp].md`

2. **Missing Main Content**
   - Check if the site requires JavaScript
   - Check if content is behind authentication
   - Try using the `--selector` option to target specific content

3. **Incomplete Table Extraction**
   - Check the original page structure
   - Try using the `--preserve-tables` flag
   - Complex tables may require manual cleanup

4. **Missing Images**
   - Check if images are loaded dynamically
   - Images behind authentication may not be accessible
   - Use the `--download-images` flag if implemented

5. **Broken Links**
   - Relative links might not work in the markdown
   - Use the `--fix-links` flag to convert relative links to absolute URLs

### Tips for Improving Content Extraction

1. **For dynamic websites**:
   - Increase wait time: `--wait 5`
   - Use browser mode if available: `--browser-mode`

2. **For content behind login**:
   - Always provide authentication credentials
   - For Confluence: ensure space key and authentication are correct

3. **For large websites**:
   - Control crawl depth: `--depth 2`
   - Use targeted crawling: `--include-pattern "/docs/.*"`

4. **For sites with specific content**:
   - Use CSS selectors: `--selector "article.main-content"`
   - Exclude unwanted sections: `--exclude-selector "div.ads, div.sidebar"`

## Troubleshooting

### Common Errors and Solutions

#### AttributeError

**Error**: `AttributeError: 'CrawlResult' object has no attribute 'get'` or similar

**Cause**: This typically occurs when the crawler tries to access a property using dictionary-style access on an object that doesn't support it. Common reasons include:
- The crawl4ai library returns a CrawlResult object instead of a dictionary
- The page structure differs from expected
- The page didn't load correctly
- Content is dynamically generated via JavaScript

**Solutions**:
1. Check if the URL is valid and accessible in a browser
2. Ensure the page doesn't require JavaScript to render
3. Try using the `--verbose` flag for more detailed logs
4. Check the log file for details about the attribute
5. Increase timeout: `--timeout 60`
6. Update to the latest version of the scraper which handles CrawlResult objects properly

#### ImportError

**Error**: `ImportError: No module named 'crawl4ai'` or similar

**Cause**: Missing dependencies.

**Solutions**:
1. Run: `pip install -r requirements.txt`
2. Run: `python -m src.setup`
3. Check virtual environment activation
4. Try: `python -m playwright install chromium`
5. Run the deploy.sh script: `./deploy.sh`

#### ConnectionError

**Error**: `ConnectionError: Failed to establish a connection`

**Cause**: Connection issues.

**Solutions**:
1. Check internet connection
2. Verify URL accessibility
3. Configure proxy settings if needed
4. Add delays: `--delay 2`
5. Check robots.txt compliance

#### TimeoutError

**Error**: `TimeoutError: Operation timed out`

**Solutions**:
1. Increase timeout: `--timeout 60`
2. Check internet connection speed
3. Use more targeted scraping
4. Try during off-peak hours

#### AuthenticationError

**Error**: `AuthenticationError: Failed to authenticate`

**Solutions**:
1. Verify credentials
2. Check Confluence space key
3. Verify account access
4. Check for IP restrictions

### Debugging Techniques

#### Using Logs

1. **Finding logs**: Check the `logs` directory
2. **Increasing verbosity**: Use the `--verbose` flag
3. **Analyzing logs**:
   ```bash
   # View the log file
   cat logs/scraper_TIMESTAMP.log
   
   # Search for errors
   grep "ERROR" logs/scraper_TIMESTAMP.log
   ```

#### Structure Inspection

1. Use `--output-raw` if available to save raw HTML
2. Examine the HTML structure
3. For Confluence pages, verify correct space key and page ID
4. Compare browser view with scraper expectations

## Logging Configuration

The scraper uses Python's logging module to provide detailed information about its operation. Understanding log levels helps you diagnose issues:

### Log Levels

- **DEBUG**: Detailed information, typically of interest only when diagnosing problems
  - Example: `DEBUG - Extracted 15 links from page`
  - When to use: When troubleshooting detailed behavior or developing the scraper

- **INFO**: Confirmation that things are working as expected
  - Example: `INFO - Successfully crawled https://example.com`
  - When to use: For normal operation to track progress

- **WARNING**: An indication that something unexpected happened, but the scraper can continue
  - Example: `WARNING - Table structure simplified due to complex formatting`
  - When to use: To identify potential issues that don't prevent operation

- **ERROR**: Due to a more serious problem, the scraper couldn't perform a specific function
  - Example: `ERROR - Failed to crawl https://example.com/page: Connection refused`
  - When to use: To identify failures that prevent specific operations

- **CRITICAL**: A serious error indicating the program may be unable to continue running
  - Example: `CRITICAL - Authentication system failure`
  - When to use: For severe issues that prevent the scraper from functioning

### Configuring Logging

You can configure logging through:

1. **Command line**: Use the `--verbose` flag for more detailed logs
2. **Configuration file**: Set logging level in `config/default_config.yml`:
   ```yaml
   logging:
     level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
     file: "./logs/scraper.log"
     console: true
   ```

3. **Log file location**: By default, logs are stored in the `logs` directory with timestamps

## Running Tests

The project includes a comprehensive test suite. To run the tests:

### Running All Tests

```bash
python -m unittest discover tests
```

### Running Specific Test Modules

```bash
# Run file manager tests
python -m unittest tests.test_file_manager

# Run processor tests
python -m unittest tests.test_processor

# Run crawler tests
python -m unittest tests.test_crawler

# Run integration tests
python -m unittest tests.test_integration
```

### Running with Coverage

If you have the coverage package installed:

```bash
# Run tests with coverage
coverage run -m unittest discover tests

# Generate coverage report
coverage report

# Generate HTML coverage report
coverage html
```

## Deployment

The project includes a deployment script (`deploy.sh`) that automates the installation and update process:

```bash
./deploy.sh
```

This script:
1. Creates a backup of the current code
2. Sets up a Python virtual environment if needed
3. Updates pip and setuptools
4. Installs all dependencies from requirements.txt
5. Adds additional dependencies like html2text if needed
6. Installs the package in development mode
7. Runs tests if pytest is available
8. Runs the setup script
9. Clears Python cache files
10. Tests the scrapemd command

You can use this script to quickly set up the project on a new system or update an existing installation.

## Development

See [PROJECT_PLAN.md](PROJECT_PLAN.md) for detailed information about the project architecture and development plan.

## License

MIT
